{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1K_Mbc34IVstkQr3FMTBM5smbqBwsMq5t",
      "authorship_tag": "ABX9TyM9YRlnh+jERHivV0yDZWNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beltranJ/StandfordCS231n/blob/main/Image_classifier_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Quick image classifier with CIFAR-10 Dataset using a custom CNN architecture. As a result I got a **73% accuracy at test time**."
      ],
      "metadata": {
        "id": "DCYwvF_jG8iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht8U7G2rKRz8",
        "outputId": "c4901b9e-e243-4441-e141-9113089c524f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_root = r\"drive/MyDrive/ml/standford cs231n/assignment2/cs231n/datasets\"\n"
      ],
      "metadata": {
        "id": "4K_iyOUrKvty"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data loading and normalization"
      ],
      "metadata": {
        "id": "LVskm-CwIw_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "to_tensor = T.ToTensor()\n",
        "\n",
        "# means and stds were computed in advanced\n",
        "ch_means = [0.4914, 0.4822, 0.4465]\n",
        "ch_stds = [0.2470, 0.2435, 0.2616]\n",
        "\n",
        "normalize = T.Normalize(mean=ch_means, std=ch_stds)\n",
        "\n",
        "class tensor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.t = T.ToTensor()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return self.t(x)\n",
        "        \n",
        "\n",
        "transforms = torch.nn.Sequential(\n",
        "                                 tensor(),\n",
        "                                 T.Normalize(mean=ch_means, std=ch_stds)\n",
        "                                 )\n",
        "\n",
        "\n",
        "\n",
        "dset = CIFAR10(root = cifar_root,\n",
        "               transform = transforms,\n",
        "               download = False)"
      ],
      "metadata": {
        "id": "NEwsQhlSLFe-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class datasetSplit(Dataset):\n",
        "    \n",
        "    def __init__(self, list_IDs, dataset):\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.list_IDs = list_IDs\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.list_IDs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        idx = self.list_IDs[index]\n",
        "        X, y = self.dataset[idx]\n",
        "    \n",
        "        return X, y\n",
        "\n",
        "        \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "params = {\n",
        "          \"batch_size\": 25,\n",
        "          \"shuffle\":  True,\n",
        "          \"num_workers\": 2 # increases efficiency\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "train_len = int(len(dset)*0.8)\n",
        "idxs_total = torch.randperm(len(dset))\n",
        "\n",
        "idxs_train = idxs_total[:train_len]\n",
        "idxs_val = idxs_total[train_len:]\n",
        "\n",
        "dset_train = datasetSplit(list_IDs=idxs_train, dataset=dset)\n",
        "dset_val = datasetSplit(list_IDs=idxs_val, dataset=dset)\n",
        "\n",
        "dloader_train = DataLoader(dataset = dset_train, **params)\n",
        "dloader_val = DataLoader(dataset = dset_val, **params)"
      ],
      "metadata": {
        "id": "xNQNTHXmS4T0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "tX9jMcMgH0Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import relu\n",
        "\n",
        "class classifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.c1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"same\")\n",
        "        self.c2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"same\")\n",
        "        self.c3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=\"same\")\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=1)        \n",
        "        self.fc1 = nn.Linear(in_features=8*8*256, out_features=1000)\n",
        "        self.fc2 = nn.Linear(in_features = 1000, out_features=100)\n",
        "        self.fc3 = nn.Linear(in_features = 100, out_features=10)\n",
        "\n",
        "        self.m1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.m2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.c1(x)\n",
        "        x = relu(x)\n",
        "        x = self.c2(x)\n",
        "        x = relu(x)\n",
        "        x = self.m1(x)\n",
        "        x = self.c3(x)\n",
        "        x = relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.m2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = relu(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "        "
      ],
      "metadata": {
        "id": "Q9jFDWgES--6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ry1pGaElHpoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"working with \", device)\n",
        "\n",
        "epochs = 15\n",
        "print_every =  530\n",
        "\n",
        "p = {\"lr\": 1.0e-3, \"weight_decay\": 0}\n",
        "\n",
        "model = classifier().to(device=device)\n",
        "cross_entropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = optim.Adam(params=model.parameters(), **p)\n",
        "\n",
        "model.train(mode=True)\n",
        "best_val = 0\n",
        "\n",
        "for e in range(epochs):\n",
        "    for i, (x_batch, y_batch) in enumerate(dloader_train):\n",
        "        \n",
        "        x_batch = x_batch.to(device=device)\n",
        "        y_batch = y_batch.to(device=device).to(dtype=torch.long)\n",
        "        \n",
        "        scores = model(x_batch)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = cross_entropy(scores, y_batch)\n",
        "        loss.backward()\n",
        "    \n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1)  % print_every == 0: \n",
        "            num_correct = 0\n",
        "            num_total = 0\n",
        "            with torch.no_grad():\n",
        "                \n",
        "                for j, (x_batch, y_batch) in enumerate(dloader_val):\n",
        "                    \n",
        "                    x_batch, y_batch = x_batch.to(device=device), y_batch.to(device=device, dtype=torch.long)\n",
        "                    \n",
        "                    scores = model(x_batch)\n",
        "                    predictions = torch.argmax(scores, dim=-1)\n",
        "                    num_correct += (predictions == y_batch).sum()\n",
        "                    num_total += y_batch.shape[0]\n",
        "                    \n",
        "                accuracy = num_correct/num_total\n",
        "                \n",
        "                if accuracy > best_val: \n",
        "                    best_val = accuracy\n",
        "                    best_model = model\n",
        "                \n",
        "                print(\"epoch %d/%d, validation accuracy %.4f, loss: %.2f \" % (e+1, epochs, accuracy, loss.item()) )\n",
        "                         \n",
        "        \n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AGpXiYCTEsO",
        "outputId": "e67a3009-c621-4a00-d515-ef110056c2ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working with  cuda\n",
            "epoch 1/15, validation accuracy 0.5285, loss: 1.44 \n",
            "epoch 1/15, validation accuracy 0.6014, loss: 1.13 \n",
            "epoch 1/15, validation accuracy 0.6626, loss: 0.90 \n",
            "epoch 2/15, validation accuracy 0.6672, loss: 0.90 \n",
            "epoch 2/15, validation accuracy 0.6936, loss: 1.10 \n",
            "epoch 2/15, validation accuracy 0.7137, loss: 0.64 \n",
            "epoch 3/15, validation accuracy 0.7326, loss: 0.35 \n",
            "epoch 3/15, validation accuracy 0.7325, loss: 0.35 \n",
            "epoch 3/15, validation accuracy 0.7344, loss: 0.86 \n",
            "epoch 4/15, validation accuracy 0.7178, loss: 0.23 \n",
            "epoch 4/15, validation accuracy 0.7367, loss: 0.55 \n",
            "epoch 4/15, validation accuracy 0.7406, loss: 0.29 \n",
            "epoch 5/15, validation accuracy 0.7463, loss: 0.17 \n",
            "epoch 5/15, validation accuracy 0.7404, loss: 0.24 \n",
            "epoch 5/15, validation accuracy 0.7479, loss: 0.13 \n",
            "epoch 6/15, validation accuracy 0.7326, loss: 0.71 \n",
            "epoch 6/15, validation accuracy 0.7230, loss: 0.13 \n",
            "epoch 6/15, validation accuracy 0.7382, loss: 0.10 \n",
            "epoch 7/15, validation accuracy 0.7359, loss: 0.43 \n",
            "epoch 7/15, validation accuracy 0.7454, loss: 0.11 \n",
            "epoch 7/15, validation accuracy 0.7414, loss: 0.47 \n",
            "epoch 8/15, validation accuracy 0.7402, loss: 0.14 \n",
            "epoch 8/15, validation accuracy 0.7414, loss: 0.03 \n",
            "epoch 8/15, validation accuracy 0.7333, loss: 0.14 \n",
            "epoch 9/15, validation accuracy 0.7415, loss: 0.03 \n",
            "epoch 9/15, validation accuracy 0.7315, loss: 0.52 \n",
            "epoch 9/15, validation accuracy 0.7298, loss: 0.30 \n",
            "epoch 10/15, validation accuracy 0.7307, loss: 0.08 \n",
            "epoch 10/15, validation accuracy 0.7309, loss: 0.02 \n",
            "epoch 10/15, validation accuracy 0.7303, loss: 0.06 \n",
            "epoch 11/15, validation accuracy 0.7319, loss: 0.03 \n",
            "epoch 11/15, validation accuracy 0.7305, loss: 0.31 \n",
            "epoch 11/15, validation accuracy 0.7339, loss: 0.08 \n",
            "epoch 12/15, validation accuracy 0.7309, loss: 0.01 \n",
            "epoch 12/15, validation accuracy 0.7350, loss: 0.18 \n",
            "epoch 12/15, validation accuracy 0.7288, loss: 0.02 \n",
            "epoch 13/15, validation accuracy 0.7405, loss: 0.00 \n",
            "epoch 13/15, validation accuracy 0.7332, loss: 0.05 \n",
            "epoch 13/15, validation accuracy 0.7399, loss: 0.17 \n",
            "epoch 14/15, validation accuracy 0.7390, loss: 0.07 \n",
            "epoch 14/15, validation accuracy 0.7334, loss: 0.06 \n",
            "epoch 14/15, validation accuracy 0.7385, loss: 0.00 \n",
            "epoch 15/15, validation accuracy 0.7477, loss: 0.00 \n",
            "epoch 15/15, validation accuracy 0.7373, loss: 0.37 \n",
            "epoch 15/15, validation accuracy 0.7177, loss: 0.25 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model parameters (before colab console closes)\n",
        "\n",
        "torch.save(best_model.state_dict(), os.path.join(cifar_root,\"trained_model.pth\"))"
      ],
      "metadata": {
        "id": "48hLILfb8tHG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset_test = CIFAR10(root = cifar_root,\n",
        "                      train=False, \n",
        "                     transform=transforms,\n",
        "                     download=True)\n",
        "\n",
        "model = classifier()\n",
        "\n",
        "# loading model parameters \n",
        "\n",
        "model.load_state_dict(state_dict=torch.load(os.path.join(cifar_root,\"trained_model.pth\"), map_location=\"cpu\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dloader_test = DataLoader(dataset=dset_test, \n",
        "                          batch_size=64)\n",
        "correct_total = 0\n",
        "\n",
        "model.to(device=device)\n",
        "model.eval()\n",
        "\n",
        "for X_batch, y_batch in dloader_test:\n",
        "\n",
        "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "    scores = model(X_batch)\n",
        "    predicted_labels = torch.argmax(scores, dim=-1)\n",
        "    correct_total += (predicted_labels == y_batch).sum()\n",
        "    \n",
        "accuracy = correct_total/len(dset_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Z8F8udYrCa",
        "outputId": "d2594229-4315-4028-adf2-f626981ca255"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYvb58CCEvya",
        "outputId": "f668eaaa-69b3-4796-a003-252c9d1055db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7328000068664551\n"
          ]
        }
      ]
    }
  ]
}